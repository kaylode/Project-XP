{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STREET VIEW HOUSE NUMBERS DETECTION AND RECOGNITION PROJECT, USING FASTER R-CNN IN PYTORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.141531Z",
     "start_time": "2020-04-01T13:33:54.409791Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from PIL import Image\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.167358Z",
     "start_time": "2020-04-01T13:33:55.142420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = 0 #Set to 1 to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:58:42.883532Z",
     "start_time": "2020-04-01T13:58:42.875535Z"
    }
   },
   "outputs": [],
   "source": [
    "#Directories\n",
    "ANNOTATION_DIR = {\n",
    "    \"train\": \"annotation/train.json\",\n",
    "    \"val\": \"annotation/val.json\"\n",
    "}\n",
    "\n",
    "DATA_DIR = {\n",
    "    \"train\": \"data/train\",\n",
    "    \"val\": \"data/val\",\n",
    "    \"test\": \"data/test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.184307Z",
     "start_time": "2020-04-01T13:33:55.175332Z"
    }
   },
   "outputs": [],
   "source": [
    "#Data normalization and tensorization\n",
    "def data_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.192393Z",
     "start_time": "2020-04-01T13:33:55.185305Z"
    }
   },
   "outputs": [],
   "source": [
    "#Plot tensor images\n",
    "def imshow(img):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img_show = img.cpu().numpy().squeeze().transpose((1,2,0))\n",
    "    img_show = (img_show * std+mean)\n",
    "    img_show = np.clip(img_show,0,1)\n",
    "    return img_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.204282Z",
     "start_time": "2020-04-01T13:33:55.193312Z"
    }
   },
   "outputs": [],
   "source": [
    "#Custom dataset\n",
    "class SVHNDataset(data.Dataset):\n",
    "    def __init__(self, root, annotation, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.annotation = annotation\n",
    "        self.transforms = transforms\n",
    "        self.ids = os.listdir(root)\n",
    "        self.classes = [i for i in range(0,11)] #Classes: 0 - 9 digits and none\n",
    "        with open(self.annotation, 'r') as anno:\n",
    "            json_file = json.load(anno)\n",
    "        self.json_file = json_file\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        #Open annotation file in json type\n",
    "        obj_name = self.json_file[index]['filename']\n",
    "        obj_boxes = self.json_file[index]['boxes']\n",
    "        img_id = Image.open(os.path.join(self.root,obj_name))\n",
    "        \n",
    "        num_obj = len(obj_boxes)\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        #Read in bounding boxes cordinate and labels\n",
    "        for i in range(num_obj):\n",
    "            xmin =  obj_boxes[i]['left']\n",
    "            ymin =  obj_boxes[i]['top']\n",
    "            xmax =  xmin + obj_boxes[i]['width']\n",
    "            ymax =  ymin + obj_boxes[i]['height']\n",
    "            label = obj_boxes[i]['label']\n",
    "            boxes.append([xmin,ymin,xmax,ymax])\n",
    "            labels.append(label)\n",
    "        \n",
    "        #Tensorize \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        labels = torch.LongTensor(labels)\n",
    "        index = torch.Tensor([index])\n",
    "        iscrowd = torch.zeros((num_obj,), dtype=torch.int64)\n",
    "        \n",
    "        #Create dict of annotation for easy access\n",
    "        my_annotation = {}\n",
    "        my_annotation[\"boxes\"] = boxes\n",
    "        my_annotation[\"labels\"] = labels\n",
    "        my_annotation[\"image_id\"] = index\n",
    "        my_annotation[\"iscrowd\"] = iscrowd\n",
    "        my_annotation[\"area\"] = area\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms()(img_id)\n",
    "        \n",
    "        return img, my_annotation\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.213273Z",
     "start_time": "2020-04-01T13:33:55.206250Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.509441Z",
     "start_time": "2020-04-01T13:33:55.214228Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create dataset and dataloader\n",
    "if TRAIN_DATA:\n",
    "    dataset = SVHNDataset(DATA_DIR[\"train\"],ANNOTATION_DIR[\"train\"],transforms=data_transform)\n",
    "    dataloader = data.DataLoader(dataset,batch_size=1, shuffle=True, collate_fn=collate_fn, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:55.517418Z",
     "start_time": "2020-04-01T13:33:55.510436Z"
    }
   },
   "outputs": [],
   "source": [
    "if TRAIN_DATA:\n",
    "    CLASSES = dataset.classes\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    TRAINING_SAMPLES =len(dataset)\n",
    "    print(f\"Number of training samples: {TRAINING_SAMPLES}\")\n",
    "    print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "    print(f\"Classes: {CLASSES}\") #label '10' is for digit '0' and '0' is for none digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:56.617986Z",
     "start_time": "2020-04-01T13:33:55.518415Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Visualize 10 training samples\n",
    "def visualize_samples():\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Training Samples\")\n",
    "    for id,(img,anno) in enumerate(dataloader): \n",
    "        if id==9:\n",
    "            break\n",
    "        ax = fig.add_subplot(3,3,id+1)\n",
    "        for label, box in zip(anno[0][\"labels\"], anno[0][\"boxes\"]):\n",
    "            rect = patches.Rectangle((box[0],box[1]),box[2]-box[0],box[3]-box[1],fill=False,ec=\"red\",lw=3)\n",
    "            plt.text(box[0],box[1]-3,int(label.numpy()),color=\"red\",fontsize=15, fontweight='bold')\n",
    "            ax.add_patch(rect)\n",
    "        img = imshow(img[0])\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n",
    "#visualize_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:58.641492Z",
     "start_time": "2020-04-01T13:33:56.618984Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FasterRCNN(\n",
      "  (transform): GeneralizedRCNNTransform()\n",
      "  (backbone): BackboneWithFPN(\n",
      "    (body): IntermediateLayerGetter(\n",
      "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (bn1): FrozenBatchNorm2d()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): Bottleneck(\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): FrozenBatchNorm2d()\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): Bottleneck(\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): FrozenBatchNorm2d()\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): FrozenBatchNorm2d()\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): FrozenBatchNorm2d()\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      )\n",
      "      (extra_blocks): LastLevelMaxPool()\n",
      "    )\n",
      "  )\n",
      "  (rpn): RegionProposalNetwork(\n",
      "    (anchor_generator): AnchorGenerator()\n",
      "    (head): RPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): RoIHeads(\n",
      "    (box_roi_pool): MultiScaleRoIAlign()\n",
      "    (box_head): TwoMLPHead(\n",
      "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNPredictor(\n",
      "      (cls_score): Linear(in_features=1024, out_features=11, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=44, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Create Faster R-CNN model\n",
    "def instance_segmentation_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    #Edit the last layer \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = instance_segmentation_model(11)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster R-CNN model in Pytorch receives input as a list of image and a list of annotation dicts, then outputs losses when in training mode or the target annotation prediction when in evaluate mode. More detail on official Pytorch website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:33:58.649481Z",
     "start_time": "2020-04-01T13:33:58.642461Z"
    }
   },
   "outputs": [],
   "source": [
    "#Training process\n",
    "def train(model,dataloader):\n",
    "    EPOCHS = 1\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_list = []\n",
    "    print(\"Start training...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for id, (imgs,anno) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Put images and annotations in list type\n",
    "            imgs = list(image.to(device) for image in imgs)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in anno]\n",
    "            \n",
    "            #Model outputs are losses when in training mode\n",
    "            loss_dict = model(imgs, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            \n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(losses)\n",
    "            print(f'Iteration: [{id}/{len(dataloader)}], Loss: {losses}')\n",
    "            \n",
    "            #Save model at every k iterations\n",
    "            if id %3000 == 0:\n",
    "                torch.save(model.state_dict(), \"model/model.pth\")\n",
    "        torch.save(model.state_dict(), \"model/model.pth\")\n",
    "    print(\"Training Completed!\")\n",
    "    \n",
    "    return model, loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In evaluating mode, the model gives out predicted annotation (bounding boxes, labels, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:35:40.820669Z",
     "start_time": "2020-04-01T13:35:40.815654Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, img):\n",
    "    #Set model into evaluate mode\n",
    "    model.eval()\n",
    "    \n",
    "    #Note: PIL image dimension is h,w\n",
    "    h,w = img.size\n",
    "    img = data_transform()(img)\n",
    "    \n",
    "    #Tensor view is w,h\n",
    "    imgs = Variable(img).view(-1,3,w,h).to(device)\n",
    "    \n",
    "    #Predict\n",
    "    with torch.no_grad():\n",
    "        outputs = model(imgs)\n",
    "    if torch.cuda.is_available():\n",
    "        del imgs\n",
    "        torch.cuda.empty_cache() \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T13:35:42.247355Z",
     "start_time": "2020-04-01T13:35:42.045878Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if TRAIN_DATA:\n",
    "    model, loss_list = train(model,dataloader)\n",
    "else:\n",
    "    #if not train, load trained model\n",
    "    model.load_state_dict(torch.load(\"model/model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:07:13.753850Z",
     "start_time": "2020-04-01T14:07:13.744874Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing model\n",
    "def visualize_test(NUM_PIC=4):\n",
    "    thresh = 0.87 #Threshhold to ensure that is a digit\n",
    "    fig = plt.figure(figsize=(15,15))\n",
    "    a = np.random.randint(13000)\n",
    "    path = os.listdir(DATA_DIR[\"test\"])\n",
    "    for id,i in enumerate(path[a:a+NUM_PIC]):\n",
    "\n",
    "        fig.add_subplot(NUM_PIC/2,2,id+1)\n",
    "        \n",
    "        #Open image\n",
    "        img = Image.open(os.path.join(DATA_DIR[\"test\"],i))\n",
    "        \n",
    "        #Predict each image at a time\n",
    "        prediction = predict(model,img)\n",
    "        \n",
    "        #Get top scores that pass the threshold\n",
    "        scores = prediction[0][\"scores\"]\n",
    "        scores = scores[scores >= thresh]\n",
    "        num_objs = len(scores)\n",
    "        \n",
    "        #Get predicted bbox and label\n",
    "        bboxes = prediction[0][\"boxes\"][:num_objs]\n",
    "        labels = prediction[0][\"labels\"][:num_objs]\n",
    "        plt.imshow(img)\n",
    "        ax = plt.gca()\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(i)\n",
    "        for box, label,score in zip(bboxes,labels,scores):\n",
    "            x,y,x2,y2 = box\n",
    "            label = str(label.cpu().numpy()) #+\": \" + \"{:.3f}\".format(score.cpu().numpy())\n",
    "            \n",
    "            #Draw bboxes\n",
    "            rect = patches.Rectangle((x,y),x2-x,y2-y,color=\"red\",lw=3,fill=False)\n",
    "            #Print label\n",
    "            plt.text(x,y-2,label,color=\"red\",fontsize=10, fontweight='bold')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            #These \"del\"s for flush cuda memory, prevent it from overloading\n",
    "            if torch.cuda.is_available():\n",
    "                del x,y,x2,y2,box,label,score\n",
    "                torch.cuda.empty_cache()\n",
    "        if torch.cuda.is_available():\n",
    "            del prediction,bboxes, labels,scores\n",
    "            torch.cuda.empty_cache() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-01T14:11:43.047568Z",
     "start_time": "2020-04-01T14:11:40.912149Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.jpg', '2.jpg']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1080x1080 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_test(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
