{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:49.091606Z",
     "start_time": "2020-04-08T08:20:46.394707Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.data as data\n",
    "import pandas as pd\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:49.127487Z",
     "start_time": "2020-04-08T08:20:49.095574Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda: 0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:49.149456Z",
     "start_time": "2020-04-08T08:20:49.129482Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 2020\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:49.154465Z",
     "start_time": "2020-04-08T08:20:49.150426Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/train.csv\"\n",
    "PREPROCESS_DATA_DIR = \"data/preprocessed.csv\"\n",
    "PREPROCESS_TEST_DIR = \"data/preprocessed_test2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:49.856219Z",
     "start_time": "2020-04-08T08:20:49.155413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "traindata = pd.read_csv(DATA_DIR)\n",
    "print(traindata.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:49.860209Z",
     "start_time": "2020-04-08T08:20:49.857217Z"
    }
   },
   "outputs": [],
   "source": [
    "MAKE_PREPROCESSED_DATA = 0\n",
    "TRAIN_DATA = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:51.509703Z",
     "start_time": "2020-04-08T08:20:49.862204Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "spacy_tokenizer = torchtext.data.utils.get_tokenizer('spacy')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def preprocessing(text):\n",
    "  \n",
    "    def tokenizer(text):\n",
    "        text = str.split(text)\n",
    "        return text\n",
    "    \n",
    "    def remove_punctuations(sentence):\n",
    "        result = \"\".join([w if w not in punctuations and not w.isdigit() else \" \" for w in sentence])\n",
    "        return result\n",
    "    \n",
    "    def word_lemmatizer(sentence):\n",
    "        result = lemmatizer.lemmatize(sentence)\n",
    "        return result\n",
    "    \n",
    "    def word_lowercase(sentence):\n",
    "        return sentence.lower()\n",
    "    \n",
    "    def remove_URL(text):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        html=re.compile(r'<.*?>')\n",
    "        text = html.sub(r'',text)\n",
    "        text = url.sub(r'',str(text))\n",
    "        return text\n",
    "  \n",
    "    def remove_newline(text):\n",
    "        return text.rstrip(\"\\n\")\n",
    "    \n",
    "    def clean(sentence):\n",
    "        result = []\n",
    "        sentence = remove_newline(sentence)\n",
    "        sentence = remove_URL(sentence)\n",
    "        sentence = word_lowercase(sentence)\n",
    "        sentence = word_lemmatizer(sentence)\n",
    "        sentence = remove_punctuations(sentence)\n",
    "        sentence = tokenizer(sentence)\n",
    "\n",
    "        result = \" \".join(sentence)\n",
    "        return result\n",
    "    \n",
    "    #result = generate_bigrams(result)  \n",
    "    text = clean(text)\n",
    "    if text == \"\":\n",
    "        text = \"None\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:51.516581Z",
     "start_time": "2020-04-08T08:20:51.511593Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "if MAKE_PREPROCESSED_DATA:\n",
    "    with open(DATA_DIR, \"r\", encoding=\"utf8\") as in_csv, open(PREPROCESS_DATA_DIR, \"w\", newline=\"\", encoding=\"utf8\") as out_csv:\n",
    "        reader = csv.reader(in_csv)\n",
    "        writer = csv.writer(out_csv)\n",
    "        next(reader, None) # Skip header\n",
    "        for row in tqdm(reader):\n",
    "            row[1] = preprocessing(row[1])\n",
    "            try:\n",
    "                writer.writerow(row)\n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:53.430235Z",
     "start_time": "2020-04-08T08:20:51.518575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length that a single input can have using this model: 512 words\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased'] \n",
    "print(f\"Maximum length that a single input can have using this model: {max_input_length} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:53.434194Z",
     "start_time": "2020-04-08T08:20:53.431202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using inititated tokens from transformers\n",
    "init_token_idx = tokenizer.cls_token_id\n",
    "eos_token_idx = tokenizer.sep_token_id\n",
    "pad_token_idx = tokenizer.pad_token_id\n",
    "unk_token_idx = tokenizer.unk_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:53.442173Z",
     "start_time": "2020-04-08T08:20:53.435192Z"
    }
   },
   "outputs": [],
   "source": [
    "def mytokenizer(sentence):\n",
    "    tokens = str.split(sentence)\n",
    "    tokens = tokens[:max_input_length-2] # BERT model appends two tokens to each sequence\n",
    "                                         # one at the beginning, one at the end\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:20:53.450152Z",
     "start_time": "2020-04-08T08:20:53.444169Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT = data.Field(batch_first = True,\n",
    "                  use_vocab= False, # so we dont need to build vocabulary from training data\n",
    "                  tokenize = mytokenizer,\n",
    "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
    "                  init_token = init_token_idx,\n",
    "                  eos_token = eos_token_idx,\n",
    "                  pad_token = pad_token_idx,\n",
    "                  unk_token = unk_token_idx,\n",
    "                  stop_words=stopwords_list)\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "ID2 = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:11.471595Z",
     "start_time": "2020-04-08T08:20:53.451149Z"
    }
   },
   "outputs": [],
   "source": [
    "FIELDS = [[\"id\",None], [\"text\", TEXT], [\"toxic\",LABEL],[\"s_toxic\",LABEL],\n",
    "          [\"obscene\",LABEL],[\"threat\",LABEL],[\"insult\",LABEL],[\"id_hate\",LABEL]]\n",
    "TEST_FIELDS = [[\"id\",ID2], [\"text\", TEXT]]\n",
    "trainset = data.TabularDataset(PREPROCESS_DATA_DIR,\n",
    "                              format = \"csv\",\n",
    "                              fields=FIELDS,\n",
    "                              skip_header=True)\n",
    "testset = data.TabularDataset(PREPROCESS_TEST_DIR,\n",
    "                             format=\"csv\",\n",
    "                             fields=TEST_FIELDS,\n",
    "                             skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:12.856066Z",
     "start_time": "2020-04-08T08:21:11.472519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in training set: 30522\n",
      "Unique labels in training set: 2\n"
     ]
    }
   ],
   "source": [
    "LABEL.build_vocab(trainset)\n",
    "ID2.build_vocab(testset)\n",
    "print(f\"Unique words in training set: {len(tokenizer.vocab)}\")\n",
    "print(f\"Unique labels in training set: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:12.879007Z",
     "start_time": "2020-04-08T08:21:12.857063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0000247867823ef7', 1), ('00013b17ad220c46', 1), ('00017563c3f7919a', 1), ('00017695ad8997eb', 1), ('0001ea8717f6de06', 1), ('00024115d4cbde0f', 1), ('000247e83dcc1211', 1), ('00025358d4737918', 1), ('00026d1092fe71cc', 1), ('0002eadc3b301559', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(ID2.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:12.890003Z",
     "start_time": "2020-04-08T08:21:12.881001Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_iter = data.BucketIterator(trainset,\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           device = device)\n",
    "test_iter = data.BucketIterator(testset,\n",
    "                                batch_size= BATCH_SIZE,\n",
    "                                shuffle=False,\n",
    "                                device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:17.058121Z",
     "start_time": "2020-04-08T08:21:12.891001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained(\"pretrained_model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:17.068068Z",
     "start_time": "2020-04-08T08:21:17.059090Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers=n_layers,\n",
    "                          bidirectional=bidirectional,\n",
    "                          batch_first=True,\n",
    "                          dropout=0 if n_layers < 2 else dropout)\n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout,inplace=False)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        # text = [batch size, sent len]\n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text)[0]\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        # hidden = [n layers * n directions, batch size, emb dim]\n",
    "        hidden_tmp = hidden.clone()\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(\n",
    "                torch.cat((hidden_tmp[-2, :, :], hidden_tmp[-1, :, :]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden_tmp[-1, :, :])\n",
    "        # hidden = [batch size, hid dim]\n",
    "        output = self.out(hidden)\n",
    "        # output = [batch size, out dim]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:19.336319Z",
     "start_time": "2020-04-08T08:21:17.069064Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTGRUSentiment(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (rnn): GRU(768, 256, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
      "  (out): Linear(in_features=512, out_features=6, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 6\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTGRUSentiment(bert,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         BIDIRECTIONAL,\n",
    "                         DROPOUT)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 2e-4, weight_decay = 1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:19.342303Z",
     "start_time": "2020-04-08T08:21:19.337345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,761,734 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Freeze the BERT model's parameters\n",
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:19.349285Z",
     "start_time": "2020-04-08T08:21:19.343301Z"
    }
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:19.357264Z",
     "start_time": "2020-04-08T08:21:19.350311Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_labels(batch):\n",
    "    toxic = batch.toxic.unsqueeze(1)\n",
    "    s_toxic = batch.s_toxic.unsqueeze(1)\n",
    "    obscene = batch.obscene.unsqueeze(1)\n",
    "    threat = batch.threat.unsqueeze(1)\n",
    "    insult = batch.insult.unsqueeze(1)\n",
    "    id_hate = batch.id_hate.unsqueeze(1)\n",
    "    labels = torch.cat((toxic,s_toxic,obscene,\n",
    "                        threat,insult,id_hate),dim=1)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:19.366267Z",
     "start_time": "2020-04-08T08:21:19.360257Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_step(model, optimizer, criterion, batch):\n",
    "    model.train()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    text = batch.text\n",
    "    labels = get_labels(batch)\n",
    "\n",
    "    outputs = model(text)\n",
    "    loss = criterion(outputs,labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:16:01.325318Z",
     "start_time": "2020-04-08T05:59:11.289536Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch: [1/3] | Iterations: [1/9974] | Training loss: 0.693\n",
      "Epoch: [1/3] | Iterations: [401/9974] | Training loss: 0.141\n",
      "Epoch: [1/3] | Iterations: [801/9974] | Training loss: 0.102\n",
      "Epoch: [1/3] | Iterations: [1201/9974] | Training loss: 0.009\n",
      "Epoch: [1/3] | Iterations: [1601/9974] | Training loss: 0.065\n",
      "Epoch: [1/3] | Iterations: [2001/9974] | Training loss: 0.035\n",
      "Epoch: [1/3] | Iterations: [2401/9974] | Training loss: 0.023\n",
      "Epoch: [1/3] | Iterations: [2801/9974] | Training loss: 0.002\n",
      "Epoch: [1/3] | Iterations: [3201/9974] | Training loss: 0.054\n",
      "Epoch: [1/3] | Iterations: [3601/9974] | Training loss: 0.002\n",
      "Epoch: [1/3] | Iterations: [4001/9974] | Training loss: 0.051\n",
      "Epoch: [1/3] | Iterations: [4401/9974] | Training loss: 0.155\n",
      "Epoch: [1/3] | Iterations: [4801/9974] | Training loss: 0.063\n",
      "Epoch: [1/3] | Iterations: [5201/9974] | Training loss: 0.011\n",
      "Epoch: [1/3] | Iterations: [5601/9974] | Training loss: 0.017\n",
      "Epoch: [1/3] | Iterations: [6001/9974] | Training loss: 0.060\n",
      "Epoch: [1/3] | Iterations: [6401/9974] | Training loss: 0.012\n",
      "Epoch: [1/3] | Iterations: [6801/9974] | Training loss: 0.073\n",
      "Epoch: [1/3] | Iterations: [7201/9974] | Training loss: 0.029\n",
      "Epoch: [1/3] | Iterations: [7601/9974] | Training loss: 0.025\n",
      "Epoch: [1/3] | Iterations: [8001/9974] | Training loss: 0.011\n",
      "Epoch: [1/3] | Iterations: [8401/9974] | Training loss: 0.002\n",
      "Epoch: [1/3] | Iterations: [8801/9974] | Training loss: 0.075\n",
      "Epoch: [1/3] | Iterations: [9201/9974] | Training loss: 0.099\n",
      "Epoch: [1/3] | Iterations: [9601/9974] | Training loss: 0.058\n",
      "Epoch: [2/3] | Iterations: [1/9974] | Training loss: 0.087\n",
      "Epoch: [2/3] | Iterations: [401/9974] | Training loss: 0.079\n",
      "Epoch: [2/3] | Iterations: [801/9974] | Training loss: 0.067\n",
      "Epoch: [2/3] | Iterations: [1201/9974] | Training loss: 0.208\n",
      "Epoch: [2/3] | Iterations: [1601/9974] | Training loss: 0.101\n",
      "Epoch: [2/3] | Iterations: [2001/9974] | Training loss: 0.026\n",
      "Epoch: [2/3] | Iterations: [2401/9974] | Training loss: 0.010\n",
      "Epoch: [2/3] | Iterations: [2801/9974] | Training loss: 0.063\n",
      "Epoch: [2/3] | Iterations: [3201/9974] | Training loss: 0.037\n",
      "Epoch: [2/3] | Iterations: [3601/9974] | Training loss: 0.061\n",
      "Epoch: [2/3] | Iterations: [4001/9974] | Training loss: 0.031\n",
      "Epoch: [2/3] | Iterations: [4401/9974] | Training loss: 0.059\n",
      "Epoch: [2/3] | Iterations: [4801/9974] | Training loss: 0.036\n",
      "Epoch: [2/3] | Iterations: [5201/9974] | Training loss: 0.063\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-7be7d098369c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m400\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-15582656fc6c>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, optimizer, criterion, batch)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "if TRAIN_DATA:\n",
    "    EPOCHS = 3\n",
    "    loss_list = []\n",
    "    print(\"Start training...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            train_loss = train_step(model,optimizer, criterion, batch)\n",
    "\n",
    "            if i%400 == 0:\n",
    "                print(f\"Epoch: [{epoch+1}/{EPOCHS}] | Iterations: [{i+1}/{len(train_iter)}] | Training loss: {train_loss:.3f}\")\n",
    "                torch.save(model.state_dict(), \"model/modelBERT2.pt\")\n",
    "    print(\"Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:33.059990Z",
     "start_time": "2020-04-08T08:21:32.660916Z"
    }
   },
   "outputs": [],
   "source": [
    "if not TRAIN_DATA:\n",
    "    model.load_state_dict(torch.load(\"model/modelBERT2.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T08:21:37.286447Z",
     "start_time": "2020-04-08T08:21:37.280440Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4)\n",
    "def predict_test(model, test_iter):\n",
    "    result = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_iter):\n",
    "            batch_size = len(batch)\n",
    "            text = batch.text.view(batch_size,-1).long()\n",
    "            ids = batch.id.squeeze().cpu()\n",
    "            output = model(text)\n",
    "            output = torch.sigmoid(output).cpu()\n",
    "            for i,j in zip(ids,output):\n",
    "                result.append([ID2.vocab.itos[i.numpy()],j.numpy()])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T09:25:40.260748Z",
     "start_time": "2020-04-08T08:21:39.215262Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9573/9573 [1:04:01<00:00,  2.49it/s]\n"
     ]
    }
   ],
   "source": [
    "result = predict_test(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T09:47:23.238275Z",
     "start_time": "2020-04-08T09:47:22.055677Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 153163/153163 [00:01<00:00, 130394.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"submission2.csv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"id\"]+[\"toxic\"]+[\"severe_toxic\"]+[\"obscene\"]+[\"threat\"]+[\"insult\"]+[\"identity_hate\"])\n",
    "    for line in tqdm(result):\n",
    "        writer.writerow([line[0]] + [line[1][i] for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T09:38:42.571218Z",
     "start_time": "2020-04-08T09:38:42.562242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153163\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T09:30:49.601275Z",
     "start_time": "2020-04-08T09:30:49.104527Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment(model, tokenizer, sentence):\n",
    "    model.eval()\n",
    "    tokens = mytokenizer(sentence)\n",
    "    indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T09:30:53.274624Z",
     "start_time": "2020-04-08T09:30:51.410365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yo bitch ja rule is more succesful then you ll ever be whats up with you and hating you sad mofuckas i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me ja rule is about pride in da music man dont diss that shit on him and nothin is wrong bein like tupac he was a brother too fuckin white boys get things right next time\n"
     ]
    }
   ],
   "source": [
    "text = preprocessing(\"yo bitch ja rule is more succesful then you ll ever be whats up with you and hating you sad mofuckas i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me ja rule is about pride in da music man dont diss that shit on him and nothin is wrong bein like tupac he was a brother too fuckin white boys get things right next time\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T09:30:54.925561Z",
     "start_time": "2020-04-08T09:30:54.830842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9917, 0.4062, 0.9639, 0.0687, 0.9240, 0.4244]], device='cuda:0',\n",
      "       grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(model,tokenizer,text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
